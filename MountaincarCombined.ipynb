{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a810c7",
   "metadata": {},
   "source": [
    "SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8616d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "DISCRETE_BUCKETS = 20\n",
    "EPISODES = 30000\n",
    "DISCOUNT = 0.95\n",
    "EPISODE_DISPLAY = 500\n",
    "LEARNING_RATE = 0.1\n",
    "EPSILON = 0.5\n",
    "EPSILON_DECREMENTER = EPSILON/(EPISODES//4)\n",
    "\n",
    "#Q-Table of size DISCRETE_BUCKETS*DISCRETE_BUCKETS*env.action_space.n\n",
    "Q_TABLE = np.random.randn(DISCRETE_BUCKETS,DISCRETE_BUCKETS,env.action_space.n)\n",
    "\n",
    "#Performance measures\n",
    "ep_rewards = []\n",
    "ep_rewards_table = {'ep': [], 'avg': [], 'min': [], 'max': []}\n",
    "\n",
    "def discretized_state(state):\n",
    "\tDISCRETE_WIN_SIZE = (env.observation_space.high-env.observation_space.low)/[DISCRETE_BUCKETS]*len(env.observation_space.high)\n",
    "\tdiscrete_state = (state-env.observation_space.low)//DISCRETE_WIN_SIZE\n",
    "\treturn tuple(discrete_state.astype(np.int))\t\t#integer tuple for extracting Q table values\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "\tepisode_reward = 0\n",
    "\tdone = False\n",
    "\n",
    "\tif episode % EPISODE_DISPLAY == 0:\n",
    "\t\trender_state = True\n",
    "\telse:\n",
    "\t\trender_state = False\n",
    "\n",
    "\tcurr_discrete_state = discretized_state(env.reset())\t#initialize new state\n",
    "\tif np.random.random() > EPSILON:\n",
    "\t\taction = np.argmax(Q_TABLE[curr_discrete_state])\t# action selection from Q_Table // exploitation\n",
    "\telse:\n",
    "\t\taction = np.random.randint(0, env.action_space.n)\t# Epsilon random exploration \n",
    "\n",
    "\twhile not done:\t\t\n",
    "\t\tnew_state, reward, done, _ = env.step(action)\n",
    "\t\tnew_discrete_state = discretized_state(new_state)\n",
    "\n",
    "\t\tif np.random.random() > EPSILON:\n",
    "\t\t\tnew_action = np.argmax(Q_TABLE[new_discrete_state])\n",
    "\t\telse:\n",
    "\t\t\tnew_action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "\t\tif render_state:\n",
    "\t\t\tenv.render()\n",
    "\n",
    "\t\tif not done:\n",
    "\t\t\tcurrent_q = Q_TABLE[curr_discrete_state+(action,)]\t# Q(S,A)\t\t\t\n",
    "\t\t\tmax_future_q = Q_TABLE[new_discrete_state+(new_action,)]\t# maxQ(S,A)\n",
    "\t\t\tnew_q = current_q + LEARNING_RATE*(reward+DISCOUNT*max_future_q-current_q)\t# Q(S,A) <-- Q(S,A) + alpha[R + gamma*maxQ(S,A) - Q(S,A)]\n",
    "\t\t\tQ_TABLE[curr_discrete_state+(action,)]=new_q\t# storing new_q into Q_Table\n",
    "\t\telif new_state[0] >= env.goal_position:\t\t#win condition\n",
    "\t\t\tQ_TABLE[curr_discrete_state + (action,)] = 0\n",
    "\n",
    "\t\tcurr_discrete_state = new_discrete_state\t# S<-S'\n",
    "\t\taction = new_action\t# A<-A'\n",
    "\n",
    "\t\tepisode_reward += reward\t# reward tracker\n",
    "\n",
    "\tEPSILON = EPSILON - EPSILON_DECREMENTER\n",
    "\n",
    "\tep_rewards.append(episode_reward)\n",
    "\n",
    "\tif not episode % EPISODE_DISPLAY:\n",
    "\t\tavg_reward = sum(ep_rewards[-EPISODE_DISPLAY:])/len(ep_rewards[-EPISODE_DISPLAY:])\n",
    "\t\tep_rewards_table['ep'].append(episode)\n",
    "\t\tep_rewards_table['avg'].append(avg_reward)\n",
    "\t\tep_rewards_table['min'].append(min(ep_rewards[-EPISODE_DISPLAY:]))\n",
    "\t\tep_rewards_table['max'].append(max(ep_rewards[-EPISODE_DISPLAY:]))\n",
    "\t\t\n",
    "\t\tprint(f\"Episode:{episode} avg:{avg_reward} min:{min(ep_rewards[-EPISODE_DISPLAY:])} max:{max(ep_rewards[-EPISODE_DISPLAY:])}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "plt.plot(ep_rewards_table['ep'], ep_rewards_table['avg'], label=\"avg\")\n",
    "plt.plot(ep_rewards_table['ep'], ep_rewards_table['min'], label=\"min\")\n",
    "plt.plot(ep_rewards_table['ep'], ep_rewards_table['max'], label=\"max\")\n",
    "plt.legend(loc=4) \n",
    "plt.title('Mountain Car SARSA')\n",
    "plt.ylabel('Average reward/Episode')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c8f9d",
   "metadata": {},
   "source": [
    "Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "DISCRETE_BUCKETS = 20\n",
    "EPISODES = 30000\n",
    "DISCOUNT = 0.95\n",
    "EPISODE_DISPLAY = 500\n",
    "LEARNING_RATE = 0.1\n",
    "EPSILON = 0.5\n",
    "EPSILON_DECREMENTER = EPSILON/(EPISODES//4)\n",
    "DYNA_N = 10\n",
    "\n",
    "#Q-Table of size DISCRETE_BUCKETS*DISCRETE_BUCKETS*env.action_space.n\n",
    "Q_TABLE = np.ones((DISCRETE_BUCKETS,DISCRETE_BUCKETS,env.action_space.n))/(-100)\n",
    "Model_transitions = np.ones((DISCRETE_BUCKETS*DISCRETE_BUCKETS,env.action_space.n))/(-100)\n",
    "Model_rewards = np.ones((DISCRETE_BUCKETS*DISCRETE_BUCKETS,env.action_space.n))/(-100)\n",
    "start_dyna = False\n",
    "\n",
    "# For stats\n",
    "ep_rewards = []\n",
    "ep_rewards_table = {'ep': [], 'avg': [], 'min': [], 'max': []}\n",
    "\n",
    "def discretized_state(state):\n",
    "\tDISCRETE_WIN_SIZE = (env.observation_space.high-env.observation_space.low)/[DISCRETE_BUCKETS]*len(env.observation_space.high)\n",
    "\tdiscrete_state = (state-env.observation_space.low)//DISCRETE_WIN_SIZE\n",
    "\treturn tuple(discrete_state.astype(np.int))\t\t#integer tuple for extracting Q table values\n",
    "\n",
    "def flatten_state(state):\n",
    "\treturn state[0]*20+state[1]\n",
    "\n",
    "def unflatten_state(state):\n",
    "\treturn (int(state//20) , int(state%20))\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "\tepisode_reward = 0\n",
    "\tdone = False\n",
    "\n",
    "\tcurr_discrete_state = discretized_state(env.reset())\n",
    "\n",
    "\tif episode % EPISODE_DISPLAY == 0:\n",
    "\t\trender_state = True\n",
    "\telse:\n",
    "\t\trender_state = False\n",
    "\n",
    "\twhile not done:\n",
    "\t\tif np.random.random() > EPSILON:\n",
    "\t\t\taction = np.argmax(Q_TABLE[curr_discrete_state]) # action selection from Q_Table // exploitation\n",
    "\t\telse:\n",
    "\t\t\taction = np.random.randint(0, env.action_space.n) # Epsilon random exploration \n",
    "\t\t\n",
    "\t\tnew_state, reward, done, _ = env.step(action)\n",
    "\t\tnew_discrete_state = discretized_state(new_state)\n",
    "\t\t\n",
    "\t\t#if render_state:\n",
    "\t\t\t#env.render()\n",
    "\n",
    "\t\tif not done:\n",
    "\t\t\tmax_future_q = np.max(Q_TABLE[new_discrete_state]) # maxQ(S,A)\t\t\t\n",
    "\t\t\tcurrent_q = Q_TABLE[curr_discrete_state+(action,)]  # Q(S,A)\n",
    "\t\t\tnew_q = current_q + LEARNING_RATE*(reward + DISCOUNT*max_future_q - current_q) # Q(S,A) <-- Q(S,A) + alpha[R + gamma*maxQ(S,A) - Q(S,A)]\n",
    "\t\t\tQ_TABLE[curr_discrete_state+(action,)]=new_q  # storing new_q into Q_Table\n",
    "\n",
    "\t\t\tflattened_curr = flatten_state(curr_discrete_state) # flatten from 20,20,3 into 400,3\n",
    "\t\t\tflattened_new = flatten_state(new_discrete_state)\n",
    "\t\t\tModel_transitions[flattened_curr][action]=flattened_new # Model(s,a) <-- s',R\n",
    "\t\t\tModel_rewards[flattened_curr][action]=reward\n",
    "\n",
    "\t\t\tdyna_count=0\n",
    "\t\t\twhile(dyna_count<DYNA_N & start_dyna):\n",
    "\t\t\t\t# Randomly select visited state and action\t\t\t\t\n",
    "\t\t\t\tstate_sample = np.random.choice(np.where(np.sum(Model_transitions, axis=1)>0)[0])\n",
    "\t\t\t\tstate_sample_unflat = unflatten_state(state_sample)\n",
    "\t\t\t\taction_sample = np.random.choice(np.where(Model_rewards[state_sample]<=0)[0])\n",
    "\n",
    "\t\t\t\tnew_discrete_state_flat = Model_transitions[state_sample][action_sample] # S', R <- Model (s,a)\n",
    "\t\t\t\treward = Model_rewards[state_sample][action_sample]\n",
    "\n",
    "\t\t\t\tnew_discrete_state = unflatten_state(new_discrete_state_flat) #changing single value into int tuple\n",
    "\n",
    "\t\t\t\tmax_future_q = np.max(Q_TABLE[new_discrete_state]) # maxQ(S',A)\n",
    "\t\t\t\tcurrent_q = Q_TABLE[state_sample_unflat+(action_sample,)]  # Q(S,A)\n",
    "\t\t\t\tnew_q = current_q + LEARNING_RATE*(reward + DISCOUNT*max_future_q - current_q) # Q(S,A) <-- Q(S,A) + alpha[R + gamma*maxQ(S',A) - Q(S,A)]\n",
    "\n",
    "\t\t\t\tQ_TABLE[state_sample_unflat+(action_sample,)] = new_q\t# storing new_q into Q_Table\n",
    "\t\t\t\tdyna_count+=1\n",
    "\t\t\t\t\n",
    "\t\telif new_state[0] >= env.goal_position:  # win condition\n",
    "\t\t\tQ_TABLE[curr_discrete_state + (action,)] = 0   \n",
    "\t\t\tstart_dyna=True\n",
    "\n",
    "\t\tcurr_discrete_state = new_discrete_state # S <-- S'\n",
    "\t\tepisode_reward += reward # reward tracker\n",
    "\n",
    "\tEPSILON = EPSILON - EPSILON_DECREMENTER \n",
    "\n",
    "\tep_rewards.append(episode_reward)\n",
    "\n",
    "\tif not episode % EPISODE_DISPLAY:\n",
    "\t\tavg_reward = sum(ep_rewards[-EPISODE_DISPLAY:])/len(ep_rewards[-EPISODE_DISPLAY:])\n",
    "\t\tep_rewards_table['ep'].append(episode)\n",
    "\t\tep_rewards_table['avg'].append(avg_reward)\n",
    "\t\tep_rewards_table['min'].append(min(ep_rewards[-EPISODE_DISPLAY:]))\n",
    "\t\tep_rewards_table['max'].append(max(ep_rewards[-EPISODE_DISPLAY:]))\n",
    "\t\t\n",
    "\t\tprint(f\"Episode:{episode} avg:{avg_reward} min:{min(ep_rewards[-EPISODE_DISPLAY:])} max:{max(ep_rewards[-EPISODE_DISPLAY:])}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "plt.plot(ep_rewards_table['ep'], ep_rewards_table['avg'], label=\"avg\")\n",
    "plt.plot(ep_rewards_table['ep'], ep_rewards_table['min'], label=\"min\")\n",
    "plt.plot(ep_rewards_table['ep'], ep_rewards_table['max'], label=\"max\")\n",
    "plt.legend(loc=4)\n",
    "plt.title('Mountain Car Dyna-Q')\n",
    "plt.ylabel('Average reward/Episode')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97038925",
   "metadata": {},
   "source": [
    "Dyna-SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "#Environment values\n",
    "print(env.observation_space.high)\t#[0.6  0.07]\n",
    "print(env.observation_space.low)\t#[-1.2  -0.07]\n",
    "print(env.action_space.n)\t\t\t#3\n",
    "\n",
    "DISCRETE_BUCKETS = 20\n",
    "EPISODES = 30000\n",
    "DISCOUNT = 0.95\n",
    "EPISODE_DISPLAY = 500\n",
    "LEARNING_RATE = 0.1\n",
    "EPSILON = 0.5\n",
    "EPSILON_DECREMENTER = EPSILON/(EPISODES//4)\n",
    "DYNA_N = 10\n",
    "\n",
    "Q_TABLE = np.ones((DISCRETE_BUCKETS,DISCRETE_BUCKETS,env.action_space.n))/(-100)\n",
    "Model_transitions = np.ones((DISCRETE_BUCKETS*DISCRETE_BUCKETS,env.action_space.n))/(-100)\n",
    "Model_rewards = np.ones((DISCRETE_BUCKETS*DISCRETE_BUCKETS,env.action_space.n))/(-100)\n",
    "start_dyna = False\n",
    "\n",
    "# For stats\n",
    "ep_rewards = []\n",
    "ep_rewards_table = {'ep': [], 'avg': [], 'min': [], 'max': []}\n",
    "\n",
    "def discretised_state(state):\n",
    "\tDISCRETE_WIN_SIZE = (env.observation_space.high-env.observation_space.low)/[DISCRETE_BUCKETS]*len(env.observation_space.high)\n",
    "\tdiscrete_state = (state-env.observation_space.low)//DISCRETE_WIN_SIZE\n",
    "\treturn tuple(discrete_state.astype(np.int))\t\t#integer tuple for extracting Q table values\n",
    "\n",
    "def flatten_state(state):\n",
    "\treturn state[0]*20+state[1]\n",
    "\n",
    "def unflatten_state(state):\n",
    "\treturn (int(state//20) , int(state%20))\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "\tepisode_reward = 0\n",
    "\tdone = False\n",
    "\n",
    "\tif episode % EPISODE_DISPLAY == 0:\n",
    "\t\trender_state = True\n",
    "\telse:\n",
    "\t\trender_state = False\n",
    "\n",
    "\tcurr_discrete_state = discretised_state(env.reset())\n",
    "\tif np.random.random() > EPSILON:\n",
    "\t\taction = np.argmax(Q_TABLE[curr_discrete_state])    # action selection from Q_Table // exploitation\n",
    "\telse:\n",
    "\t\taction = np.random.randint(0, env.action_space.n)   # Epsilon random exploration\n",
    "\n",
    "\twhile not done:\t\t# new state and action chosen\n",
    "\t\tnew_state, reward, done, _ = env.step(action)\n",
    "\t\tnew_discrete_state = discretised_state(new_state)\n",
    "\n",
    "\t\tif np.random.random() > EPSILON:\n",
    "\t\t\tnew_action = np.argmax(Q_TABLE[new_discrete_state])  # new action selection from Q_Table // exploitation\n",
    "\t\telse:\n",
    "\t\t\tnew_action = np.random.randint(0, env.action_space.n)   # Epsilon random exploration\n",
    "\n",
    "\t\t#if render_state:\n",
    "\t\t\t#env.render()\n",
    "\n",
    "\t\tif not done:\n",
    "\t\t\tcurrent_q = Q_TABLE[curr_discrete_state+(action,)]\t\t\t# Q(S,A)\n",
    "\t\t\tmax_future_q = Q_TABLE[new_discrete_state+(new_action,)]    # maxQ(S,A)\n",
    "\t\t\tnew_q = current_q + LEARNING_RATE*(reward+DISCOUNT*max_future_q-current_q)  # Q(S,A) <-- Q(S,A) + alpha[R + gamma*maxQ(S,A) - Q(S,A)]\n",
    "\t\t\tQ_TABLE[curr_discrete_state+(action,)]=new_q    # storing new_q into Q_Table\n",
    "\t\t\t\n",
    "\t\t\tflattened_curr = flatten_state(curr_discrete_state) # flatten from 20,20,3 into 400,3\n",
    "\t\t\tflattened_new = flatten_state(new_discrete_state)\n",
    "\t\t\tModel_transitions[flattened_curr][action]=flattened_new # Model(s,a) <-- s',R\n",
    "\t\t\tModel_rewards[flattened_curr][action]=reward\n",
    "\t\t\tdyna_count=0\n",
    "\t\t\twhile(dyna_count<DYNA_N & start_dyna):\n",
    "\t\t\t\t# Randomly select visited state and action\t\t\t\t\n",
    "\t\t\t\tstate_sample = np.random.choice(np.where(np.sum(Model_transitions, axis=1)>0)[0])\n",
    "\t\t\t\tstate_sample_unflat = unflatten_state(state_sample)\n",
    "\t\t\t\taction_sample = np.random.choice(np.where(Model_rewards[state_sample]<=0)[0])\n",
    "\n",
    "\t\t\t\tnew_discrete_state_flat = Model_transitions[state_sample][action_sample] # s', R <- Model (s,a)\n",
    "\t\t\t\treward = Model_rewards[state_sample][action_sample]\n",
    "\n",
    "\t\t\t\tnew_discrete_state = unflatten_state(new_discrete_state_flat) #changing single value into int tuple\n",
    "\t\t\t\tcurrent_q = Q_TABLE[state_sample_unflat+(action_sample,)]  # Q(S,A)\n",
    "\t\t\t\tmax_future_q = np.max(Q_TABLE[new_discrete_state]) # maxQ(S,A)\n",
    "\t\t\t\tnew_q = current_q + LEARNING_RATE*(reward + DISCOUNT*max_future_q - current_q) # Q(S,A) <-- Q(S,A) + alpha[R + gamma*maxQ(S',A') - Q(S,A)]\n",
    "\t\t\t\tQ_TABLE[state_sample_unflat+(action_sample,)] = new_q   # storing new_q into Q_Table\n",
    "\t\t\t\tdyna_count+=1\n",
    "\n",
    "\t\telif new_state[0] >= env.goal_position:     # win condition\n",
    "\t\t\tQ_TABLE[curr_discrete_state + (action,)] = 0   # 0 instead of the -1\n",
    "\t\t\tstart_dyna=True\n",
    "\n",
    "\t\tcurr_discrete_state = new_discrete_state    # S<-S'\n",
    "\t\taction = new_action # A<-A'\n",
    "\n",
    "\t\tepisode_reward += reward    # reward tracker\n",
    "\n",
    "\tEPSILON = EPSILON - EPSILON_DECREMENTER\n",
    "\n",
    "\tep_rewards.append(episode_reward)\n",
    "\n",
    "\tif not episode % EPISODE_DISPLAY:\n",
    "\t\tavg_reward = sum(ep_rewards[-EPISODE_DISPLAY:])/len(ep_rewards[-EPISODE_DISPLAY:])\n",
    "\t\tep_rewards_table['ep'].append(episode)\n",
    "\t\tep_rewards_table['avg'].append(avg_reward)\n",
    "\t\tep_rewards_table['min'].append(min(ep_rewards[-EPISODE_DISPLAY:]))\n",
    "\t\tep_rewards_table['max'].append(max(ep_rewards[-EPISODE_DISPLAY:]))\n",
    "\t\t\n",
    "\t\tprint(f\"Episode:{episode} avg:{avg_reward} min:{min(ep_rewards[-EPISODE_DISPLAY:])} max:{max(ep_rewards[-EPISODE_DISPLAY:])}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "plt.plot(ep_rewards_table['ep'], ep_rewards_table['avg'], label=\"avg\")\n",
    "plt.plot(ep_rewards_table['ep'], ep_rewards_table['min'], label=\"min\")\n",
    "plt.plot(ep_rewards_table['ep'], ep_rewards_table['max'], label=\"max\")\n",
    "plt.legend(loc=4) \n",
    "plt.title('Mountain Car Dyna-SARSA')\n",
    "plt.ylabel('Average reward/Episode')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
